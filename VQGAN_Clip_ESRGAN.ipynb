{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkUfzT60ZZ9q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Check GPU**\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "wSfISAhyPmyp"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Install module**\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning einops transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "!pip install --upgrade google-cloud-translate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Super-Resolution\n",
        "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "#%cd Real-ESRGAN\n",
        "# Set up the environment\n",
        "!pip install basicsr\n",
        "!pip install facexlib\n",
        "!pip install gfpgan\n",
        "!pip install -r /content/Real-ESRGAN/requirements.txt\n",
        "# setup.py內部的路徑因為沒有%cd Real-ESRGAN之後會錯 記得把我修改後的setup.py取代原本的檔案\n",
        "#!python /content/Real-ESRGAN/setup.py develop\n",
        "# Download the pre-trained model\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P /content/Real-ESRGAN/experiments/pretrained_models "
      ],
      "metadata": {
        "id": "ElLkH_nS5KLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_edAkFlmUyIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/AIA/Project/VQGAN_CLIP/AI_painter_5/setup.py /content/Real-ESRGAN"
      ],
      "metadata": {
        "id": "nE0R6-cTVMwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Real-ESRGAN/setup.py develop"
      ],
      "metadata": {
        "id": "bs2FL0leekp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GvcoMPNbelMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "FhhdWrSxQhwg"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Curl VQGAN**\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_1024.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_1024.ckpt\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt\n",
        "!export GOOGLE_APPLICATION_CREDENTIALS=key.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r /content/drive/MyDrive/AIA/Project/VQGAN_CLIP/model_data/vqgan_imagenet_f16_1024.ckpt /content\n",
        "# !cp -r /content/drive/MyDrive/AIA/Project/VQGAN_CLIP/model_data/vqgan_imagenet_f16_16384.ckpt /content\n",
        "# !cp -r /content/drive/MyDrive/AIA/Project/VQGAN_CLIP/model_data/vqgan_imagenet_f16_1024.yaml /content\n",
        "# !cp -r /content/drive/MyDrive/AIA/Project/VQGAN_CLIP/model_data/vqgan_imagenet_f16_16384.yaml /content"
      ],
      "metadata": {
        "id": "YuoPke7hs2D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm result -fr\n",
        "!rm SRR* -fr"
      ],
      "metadata": {
        "id": "sHZHW64lgbwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**One Big Class**\n",
        "\n",
        "# # HyperResolution\n",
        "# import cv2\n",
        "# import glob\n",
        "# import os\n",
        "# # from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "# # from realesrgan import RealESRGANer\n",
        "# # from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n",
        "# from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/key.json\"\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import requests\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "\n",
        "userid=''\n",
        "stop_iteration =   250 #@param {type:\"number\"}\n",
        "picture_size = 600 #@param {type:\"number\"}\n",
        "display_frequency =  10 #@param {type:\"number\"}\n",
        "post_frequency = 0.1 #@param {type:\"number\"}\n",
        "post_frequency *= stop_iteration\n",
        "\n",
        "google_key = {\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"tactical-gate-336214\",\n",
        "  \"private_key_id\": \"cbf2a2364b171d96be8a1580f0f1d565deb87728\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDGYSwh8VIOOHDv\\nlilotnSjje+wwcNBBjn6PIY4PbjIq85G1ScqDSY+1rcWY6HMfPh22dJSbVD9wnyF\\nbQQDkeiI7wl4iSdeZj4RmMywpcYGtXPyvZKCMYU+I4oPekqqPRSFTC14V9PKL5ry\\nu9x3aD3re9gH1CNwR4+7uHHte+8InGq1IGgPoz9NCkdr6lymPVN28hGufSFlxE2d\\nKGz28bqT9RA9kKuAPwdXJ+nEOM7OUHHHBlcUiLtNUUl8jId/YPhuKaDdne15Z3VU\\nzkKkxLK7zsnV0LXJONO4oZlKIXf9vuCbFd0wscqfPUDPbRvxmmqJQTNh6DKcAg4y\\nLMg4kyDhAgMBAAECggEABCsStlk9wKtNXe7gQo1GIdFBwXN+TdCOtX4Z4b7UAgko\\nXMEV/TxszrRkMSD1e3R/I9/pezJ1YZoZc8Qfyd+KEM3BO6hp2KImROhSiXYHvVCI\\n6c9jd5do19aPRCpNHSLqul7ojA0wH8EIoOfbKuIAjlEGL9qsdpOV2sdeMzPD2Njn\\nSEFAuh0leMZ3LVJXR6D81e/r20F8D94g5N0Zp4douTUEIBYTJKMITA8XX5zZEgQN\\nWGpcMkDIyhUe0wD+jaVVwmzLpI7JyrIYvjVFCLRvSoIe1IImlcmR50/hbkLr1qTA\\n/2YY5rjAxaCNvtMO4rogsxtZ0w/WEiHZVVbtU+7uAQKBgQDwxf8kz5Xtg2rG4TJZ\\nLTcqHe86ROKFjIr13Ki56UbvCfmK7YzzTUByfzhRYvXpvNljLBAz8mZcT2hyRqn/\\n6p5kNVoabxeSKsFnXVRlOK3hzi5q9QXyySWaHIhwh7RI6Zx6jEAkQ5+Vyr+kz3VR\\nNFYTwz9uF6ML7wD3bOJrOWOe4QKBgQDS7NcZyGdUQPm6bvUKvwVL8jtvrx3FaKgZ\\nwljsPIttuK6XloR5yD1O7mSxyEDeOgK+4WHQl73YcWpTLoFsU0/9mbLgg3ywPCSA\\nEZT/SobuOB28qWgpRzbABnFE2f6Y14vbeHn0zHBgFlz+pu//QGYTyqmiZvr3hPNK\\nr1/upunCAQKBgEIopasfvzq8WaK8TnFikz3na8y/EN9Rdv2H3HT5PJyCSSd7jrds\\nCsFrY7Y5Uwke8zzk5q7fdHD5AOIluKCxf0RZxKBu1jM7vFkCtCX8JQte9DI/kZrc\\n2pA2Nud04n0GdBDYaOeODVPDmlVXDA18LArSI6PEqvCkg8d5C94art0BAoGAW9c0\\ncc2I8G66TisyF1PFgqQerSqa0/IV/+FXxsU2ELhxjR/E3yIJT+0NezuuwxSPTX3l\\nkWYBC9WWblG5mOlN0yaxdIDMMhB99CWzstVIm0Fj8VnyOMcBV+t2NnyVFwgDpbR3\\npde+tpgGNfuKVKI90DBWXJhqfowtQz/Jjaiv8AECgYAKjtEs0VmQzsxHZSM+Ik5I\\n1KkJYzw6PwUTxnGboDJPd1SXDMsOb8vyyA8Jbjzb/z/xV4DSnP3qr/wtygF/N29a\\n9kJxuuDGyXzvAn+jMcr5QELAODs/b5jtkVWN7z9EfRuhe73suyxbsQeadVTFJfa2\\nmA6WZcBTQMxPWkAKUyqyjg==\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"translation-quickstart@tactical-gate-336214.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"109588682794215192512\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/translation-quickstart%40tactical-gate-336214.iam.gserviceaccount.com\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('key.json', 'w') as f:\n",
        "    json.dump(google_key, f)\n",
        "\n",
        "\n",
        "\n",
        "!export GOOGLE_APPLICATION_CREDENTIALS=key.json\n",
        "from google.cloud import translate\n",
        "def translate_text(text=\"想要的圖像\", project_id=\"tactical-gate-336214\"):\n",
        "\n",
        "    client = translate.TranslationServiceClient()\n",
        "    location = \"global\"\n",
        "    parent = f\"projects/{project_id}/locations/{location}\"\n",
        "    response = client.translate_text(\n",
        "        request={\n",
        "            \"parent\": parent,\n",
        "            \"contents\": [text],\n",
        "            \"mime_type\": \"text/plain\",\n",
        "            \"source_language_code\": \"zh-TW\",\n",
        "            \"target_language_code\": \"en-US\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    text = []\n",
        "    for trarnslate in response.translations:\n",
        "          text.append(trarnslate.translated_text)\n",
        "    return text\n",
        "  \n",
        "translate_text(\"測試\")\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 3)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        return clamp_with_grad(torch.cat(cutouts, dim=0), 0, 1)\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        print(f'checkpoint_path: {checkpoint_path}')\n",
        "        print(f'upper exist: {os.path.exists(checkpoint_path)}')\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "#------------Main Function--------------#\n",
        "\n",
        "# Generate Picture\n",
        "def generate_pic(args,first):\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print('Using device:', device)\n",
        "\n",
        "  model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "  perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "\n",
        "  cut_size = perceptor.visual.input_resolution\n",
        "  e_dim = model.quantize.e_dim\n",
        "  f = 2**(model.decoder.num_resolutions - 1)\n",
        "  make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "  n_toks = model.quantize.n_e\n",
        "  toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "  sideX, sideY = toksX * f, toksY * f\n",
        "  z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "  z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "  if args.seed is not None:\n",
        "      torch.manual_seed(args.seed)\n",
        "\n",
        "  if args.init_image:\n",
        "      pil_image = Image.open(fetch(args.init_image)).convert('RGB')\n",
        "      pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "      z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "  else:\n",
        "      one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "      z = one_hot @ model.quantize.embedding.weight\n",
        "      z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "  z_orig = z.clone()\n",
        "  z.requires_grad_(True)\n",
        "  opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  pMs = []\n",
        "\n",
        "  for prompt in args.prompts:\n",
        "      txt, weight, stop = parse_prompt(prompt)\n",
        "      embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "  for prompt in args.image_prompts:\n",
        "      path, weight, stop = parse_prompt(prompt)\n",
        "      img = resize_image(Image.open(fetch(path)).convert('RGB'), (sideX, sideY))\n",
        "      batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "      embed = perceptor.encode_image(normalize(batch)).float()\n",
        "      pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "      gen = torch.Generator().manual_seed(seed)\n",
        "      embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "      pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "  def synth(z):\n",
        "      z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "      return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def checkin(i, losses):\n",
        "      losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "      tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "      out = synth(z)\n",
        "      # Save pic #################\n",
        "      pic_name = f'/content/result/{userid}/{userid}_' + str(i) + '.png'\n",
        "      print(pic_name)\n",
        "      TF.to_pil_image(out[0].cpu()).save(pic_name)\n",
        "\n",
        "\n",
        "      # if i == 25:\n",
        "      #   with open(f\"/content/result/{userid}/{userid}_{i}.png\", \"rb\") as image_file:\n",
        "      #       file_dict = {\"ImageFile\": image_file}\n",
        "      #       response = requests.post(\"https://f662-2001-b400-e238-a81d-e149-fca0-7a37-e5ed.ngrok.io/painter/uploadImage\", files=file_dict)\n",
        "\n",
        "      \n",
        "      if i % post_frequency == 0:\n",
        "        with open(f\"/content/result/{userid}/{userid}_{i}.png\", \"rb\") as image_file:\n",
        "            \n",
        "            file_dict = {\"ImageFile\": image_file}\n",
        "            response = requests.post(\"https://f662-2001-b400-e238-a81d-e149-fca0-7a37-e5ed.ngrok.io/painter/uploadImage\", files=file_dict)\n",
        "\n",
        "\n",
        "      print(pic_name)\n",
        "      display.display(display.Image(pic_name))\n",
        "\n",
        "  def ascend_txt():\n",
        "      out = synth(z)\n",
        "    \n",
        "      iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "      result = []\n",
        "\n",
        "      if args.init_weight:\n",
        "          result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "      for prompt in pMs:\n",
        "          result.append(prompt(iii))\n",
        "\n",
        "      return result\n",
        "\n",
        "  def train(i):\n",
        "      opt.zero_grad()\n",
        "      lossAll = ascend_txt()\n",
        "      if i % args.display_freq == 0:\n",
        "          checkin(i, lossAll)\n",
        "      loss = sum(lossAll)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      with torch.no_grad():\n",
        "          z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "\n",
        "  i = 0\n",
        "  \n",
        "  try:\n",
        "      with tqdm() as pbar:\n",
        "          while True:\n",
        "              train(i)\n",
        "              pbar.update()\n",
        "              if i==stop_iteration :\n",
        "                break\n",
        "              i += 1\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "\n",
        "# Predict Noun\n",
        "def predict_noun(image):\n",
        "  import os\n",
        "  import clip\n",
        "  import torch\n",
        "  from torchvision.datasets import CIFAR100\n",
        "\n",
        "  # Load the model\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "  # Download the dataset\n",
        "  cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "  # Prepare the inputs\n",
        "  image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "  text_inputs = torch.cat([clip.tokenize(f\"a picture of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "  # Calculate features\n",
        "  with torch.no_grad():\n",
        "      image_features = model.encode_image(image_input)\n",
        "      text_features = model.encode_text(text_inputs)\n",
        "\n",
        "  # Pick the top 5 most similar labels for the image\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "  values, indices = similarity[0].topk(5)\n",
        "\n",
        "\n",
        "  # Print the result\n",
        "  print(\"\\nTop predictions:\\n\")\n",
        "  for value, index in zip(values, indices):\n",
        "      print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
        "\n",
        "def generate_video(freq,first):\n",
        "  import os\n",
        "  import numpy as np\n",
        "\n",
        "  frames = os.listdir(f'/content/result/{userid}')\n",
        "  frames = len(list(filter(lambda filename: filename.endswith(\".png\"), frames))) #Get number of jpg generated\n",
        "\n",
        "  init_frame = 0 #This is the frame where the video will start\n",
        "  last_frame = frames #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "  min_fps = 10\n",
        "  max_fps = 30\n",
        "\n",
        "  total_frames = last_frame-init_frame\n",
        "\n",
        "  #Desired video time in seconds\n",
        "  video_length = 30\n",
        "\n",
        "  frames = []\n",
        "  tqdm.write('Generating video...')\n",
        "  for i in range(init_frame,last_frame): #\n",
        "      filename = f'/content/result/{userid}/{userid}_{i*display_frequency}.png'\n",
        "      frames.append(Image.open(filename))\n",
        "\n",
        "  fps = np.clip(total_frames/video_length,min_fps,max_fps)\n",
        "\n",
        "  from subprocess import Popen, PIPE\n",
        "  p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "  for im in tqdm(frames):\n",
        "      im.save(p.stdin, 'PNG')\n",
        "  p.stdin.close()\n",
        "\n",
        "  print(\"The video is now being compressed, wait...\")\n",
        "  p.wait()\n",
        "  print(\"The video is ready\")\n",
        "\n",
        "  # Download ###################################\n",
        "  from google.colab import files\n",
        "  files.download(\"video.mp4\")\n",
        "\n",
        "  # Rename To move To folder\n",
        "  os.rename(\"/content/video.mp4\", f'/content/result/{userid}/{userid}.mp4')\n",
        "\n",
        "\n",
        "import requests \n",
        "\n",
        "import time\n",
        "\n",
        "readed = 'N'\n",
        "\n",
        "# Class AI_painter\n",
        "class AI_painter:\n",
        "  def paint(self, content):\n",
        "\n",
        "    print(content)\n",
        "\n",
        "    # response = requests.get(\"https://f662-2001-b400-e238-a81d-e149-fca0-7a37-e5ed.ngrok.io/painter/data\")\n",
        "    # content = response.json()\n",
        "    global userid \n",
        "    userid = content['userid']\n",
        "    text_to_image =content['promptTxt']\n",
        "    image_to_image = content['uploadImages']\n",
        "    readed = content['read']\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "##########################################################################\n",
        "    picture_list = image_to_image\n",
        "    print(text_to_image)\n",
        "    sentence = str(translate_text(text_to_image))\n",
        "    print(sentence)\n",
        "\n",
        "    # clip predict pic noun\n",
        "    for path in picture_list:\n",
        "      img = resize_image(Image.open(fetch(path)).convert('RGB'), (500, 500))\n",
        "\n",
        "      imgplot = plt.imshow(img)\n",
        "      plt.show()\n",
        "\n",
        "      #predict_noun(img)\n",
        "      #noun.append(predict_noun_2343(img))\n",
        "\n",
        "    # sentenceToGenerate = 'Pomeranian in ghibli studios' #@param {type:\"string\"}\n",
        "\n",
        "    for pic,first_name in zip(picture_list,range(len(picture_list))):\n",
        "        # setting\n",
        "        args = argparse.Namespace(\n",
        "            prompts=[sentence],\n",
        "            image_prompts=[pic],\n",
        "            noise_prompt_seeds=[],\n",
        "            noise_prompt_weights=[],\n",
        "            size=[picture_size, picture_size],\n",
        "            init_image=None,\n",
        "            init_weight=0.,\n",
        "            clip_model='ViT-B/32',\n",
        "            vqgan_config='vqgan_imagenet_f16_1024.yaml',\n",
        "            vqgan_checkpoint='vqgan_imagenet_f16_1024.ckpt',\n",
        "            step_size=0.05,\n",
        "            cutn=64,\n",
        "            cut_pow=1.,\n",
        "            display_freq=display_frequency,\n",
        "            seed=0,\n",
        "        )\n",
        "        \n",
        "\n",
        "        # create folder\n",
        "        import os\n",
        "\n",
        "        if not os.path.exists('/content/result/'+userid):\n",
        "          os.makedirs('/content/result/'+userid)\n",
        "\n",
        "        # main_function\n",
        "        generate_pic(args,first_name)\n",
        "\n",
        "        # generate video\n",
        "        generate_video(display_frequency,first_name)\n",
        "\n",
        "        with open(f\"/content/result/{userid}/{userid}.mp4\", \"rb\") as video_file,open(f\"/content/result/{userid}/{userid}_{stop_iteration}.png\", \"rb\") as image_file:\n",
        "          file_dict = {\"VideoFile\": video_file,\"ImageFile\": image_file}\n",
        "          response = requests.post(\"https://f662-2001-b400-e238-a81d-e149-fca0-7a37-e5ed.ngrok.io/painter/uploadVideo\", files=file_dict)"
      ],
      "metadata": {
        "id": "Qp62yBDZd8lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SuperResolution\n",
        "sys.path.append('/content/Real-ESRGAN')\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from realesrgan import RealESRGANer\n",
        "from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n",
        "from PIL import Image\n",
        "\n",
        "def SuperResolution():\n",
        "\n",
        "    # make directories\n",
        "    if not os.path.exists('/content/result/finalResult'):\n",
        "      os.makedirs('/content/result/finalResult')\n",
        "    if not os.path.exists('/content/SRResult/'):\n",
        "      os.makedirs('/content/SRResult/')\n",
        "\n",
        "    \n",
        "    # copy final result into finalResult dir\n",
        "    !cp -r /content/result/{userid}/{userid}_{stop_iteration}.png /content/result/finalResult\n",
        "\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "    netscale = 4\n",
        "\n",
        "    model_name = 'RealESRGAN_x4plus'\n",
        "    # SuperResolution input and output folders\n",
        "    SR_input = '/content/result/finalResult'\n",
        "    SR_output = '/content/SRResult/'\n",
        "\n",
        "    # determine model paths\n",
        "    model_path = os.path.join('/content/Real-ESRGAN/experiments/pretrained_models', model_name + '.pth')\n",
        "    if not os.path.isfile(model_path):\n",
        "        model_path = os.path.join('/content/Real-ESRGAN/realesrgan/weights', model_name + '.pth')\n",
        "    if not os.path.isfile(model_path):\n",
        "        raise ValueError(f'Model {model_name} does not exist.')\n",
        "\n",
        "    # restorer\n",
        "    upsampler = RealESRGANer(\n",
        "      scale=netscale,\n",
        "      model_path=model_path,\n",
        "      model=model,\n",
        "      tile=0,\n",
        "      tile_pad=10,\n",
        "      pre_pad=0,\n",
        "      half=False)\n",
        "\n",
        "    if os.path.isfile(SR_input):\n",
        "        paths = [SR_input]\n",
        "    else:\n",
        "        paths = sorted(glob.glob(os.path.join(SR_input, '*')))\n",
        "\n",
        "    ret_images = []\n",
        "    for idx, path in enumerate(paths):\n",
        "        imgname, extension = os.path.splitext(os.path.basename(path))\n",
        "        print('Testing', idx, imgname)\n",
        "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "        \n",
        "        if len(img.shape) == 3 and img.shape[2] == 4:\n",
        "            img_mode = 'RGBA'\n",
        "        else:\n",
        "            img_mode = None\n",
        "\n",
        "        try:\n",
        "            output, _ = upsampler.enhance(img, outscale=4)\n",
        "        except RuntimeError as error:\n",
        "            print('Error', error)\n",
        "            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n",
        "        else:\n",
        "            extension = extension[1:]\n",
        "            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n",
        "                extension = 'png'\n",
        "            save_path = os.path.join(SR_output, f'{imgname}_highRes.{extension}')\n",
        "            cv2.imwrite(save_path, output)\n",
        "            \n",
        "\n",
        "    #from google.colab import files\n",
        "    #files.download(f'/content/SRResult/{imgname}_highRes.png')\n",
        "\n",
        "    with open(save_path, \"rb\") as a_file:\n",
        "            file_dict = {\"ImageFile\": a_file}\n",
        "            response = requests.post(\"https://f662-2001-b400-e238-a81d-e149-fca0-7a37-e5ed.ngrok.io/painter/uploadSuperImage\", files=file_dict)"
      ],
      "metadata": {
        "id": "R1In9aBcUIrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download(f'/content/result/U11a7ed27def310fd59cce7e487b68911/U11a7ed27def310fd59cce7e487b68911.mp4')"
      ],
      "metadata": {
        "id": "vtm8UwZ3v5uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmGCCVnM-ehb"
      },
      "outputs": [],
      "source": [
        "\n",
        "while True:\n",
        "  \n",
        "  response = requests.get(\"https://f662-2001-b400-e238-a81d-e149-fca0-7a37-e5ed.ngrok.io/painter/data\")\n",
        "  content = response.json()\n",
        "\n",
        "  readed = content['read']\n",
        "  if(readed==\"N\"):\n",
        "    print(f'Received data from server:{content}')\n",
        "    test = AI_painter()\n",
        "    test.paint(content)\n",
        "\n",
        "    \n",
        "    if content['highResolution'] == 'Y':\n",
        "      SuperResolution()\n",
        "      print('highRes == Y')\n",
        "    \n",
        "  \n",
        "  print(\"Listen to server....\")\n",
        "  time.sleep(15)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fahVeWbEhreH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "VQGAN_Clip_6.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}